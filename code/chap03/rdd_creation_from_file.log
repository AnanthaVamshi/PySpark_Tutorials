./bin/spark-submit rdd_creation_from_file.py kv.txt

spark= <pyspark.sql.session.SparkSession object at 0x108fde9e8>

input path :  kv.txt

file_contents =
alex,200
bob,100
mary,700
jane,300
adel,900

rdd = kv.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
rdd.count =  5
rdd.collect() =  
[
 'alex,200', 
 'bob,100', 
 'mary,700', 
 'jane,300', 
 'adel,900'
]

pairs = PythonRDD[3] at RDD at PythonRDD.scala:48
pairs.count =  5
pairs.collect() =  
[
 ('alex', 200), 
 ('bob', 100), 
 ('mary', 700), 
 ('jane', 300), 
 ('adel', 900)
]

filtered = PythonRDD[5] at RDD at PythonRDD.scala:48
filtered.count =  2
filtered.collect() =  
[
 ('mary', 700), 
 ('adel', 900)
]