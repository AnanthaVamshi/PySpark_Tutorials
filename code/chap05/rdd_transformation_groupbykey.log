./bin/spark-submit  rdd_transformation_groupbykey.py

spark= <pyspark.sql.session.SparkSession object at 0x103856550>

list_of_tuples =  
[
 ('alex', 'Sunnyvale', 25), 
 ('alex', 'Sunnyvale', 33), 
 ('alex', 'Sunnyvale', 45), 
 ('alex', 'Sunnyvale', 63), 
 ('mary', 'Ames', 22), 
 ('mary', 'Cupertino', 66), 
 ('mary', 'Ames', 20), 
 ('bob', 'Ames', 26)
]

rdd =  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175
rdd.count() =  8
rdd.collect() =  
[
 ('alex', 'Sunnyvale', 25), 
 ('alex', 'Sunnyvale', 33), 
 ('alex', 'Sunnyvale', 45), 
 ('alex', 'Sunnyvale', 63), 
 ('mary', 'Ames', 22), 
 ('mary', 'Cupertino', 66), 
 ('mary', 'Ames', 20), 
 ('bob', 'Ames', 26)
]

rdd2 =  PythonRDD[2] at RDD at PythonRDD.scala:48
rdd2.count() =  8
rdd2.collect() =  
[
 ('alex', ('Sunnyvale', 25)), 
 ('alex', ('Sunnyvale', 33)), 
 ('alex', ('Sunnyvale', 45)), 
 ('alex', ('Sunnyvale', 63)), 
 ('mary', ('Ames', 22)), 
 ('mary', ('Cupertino', 66)), 
 ('mary', ('Ames', 20)), 
 ('bob', ('Ames', 26))
]

rdd3 =  PythonRDD[8] at RDD at PythonRDD.scala:48
rdd3.count() =  3
rdd3.collect() =  
[
 ('bob', <pyspark.resultiterable.ResultIterable object at 0x103839b90>), 
 ('alex', <pyspark.resultiterable.ResultIterable object at 0x103839c90>), 
 ('mary', <pyspark.resultiterable.ResultIterable object at 0x103872290>)
]
rdd3.mapValues().collect() =  
[
 ('bob', [('Ames', 26)]), 
 ('alex', [('Sunnyvale', 25), ('Sunnyvale', 33), ('Sunnyvale', 45), ('Sunnyvale', 63)]), 
 ('mary', [('Ames', 22), ('Cupertino', 66), ('Ames', 20)])
]







