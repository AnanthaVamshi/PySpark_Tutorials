./bin/spark-submit rdd_transformation_filter.py

spark= <pyspark.sql.session.SparkSession object at 0x103fbed90>

list_of_tuples =  
[
 ('alex', 'Sunnyvale', 25), 
 ('alex', 'Sunnyvale', 33), 
 ('mary', 'Ames', 22), 
 ('mary', 'Cupertino', 66), 
 ('jane', 'Ames', 20), 
 ('bob', 'Ames', 26)
]

rdd =  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175
rdd.count() =  6
rdd.collect() =  
[
 ('alex', 'Sunnyvale', 25), 
 ('alex', 'Sunnyvale', 33), 
 ('mary', 'Ames', 22), 
 ('mary', 'Cupertino', 66), 
 ('jane', 'Ames', 20), 
 ('bob', 'Ames', 26)
]

filtered_by_lambda =  PythonRDD[2] at RDD at PythonRDD.scala:48
filtered_by_lambda.count() =  2
filtered_by_lambda.collect() =  
[
 ('alex', 'Sunnyvale', 33), 
 ('mary', 'Cupertino', 66)
]

filtered_by_function =  PythonRDD[4] at RDD at PythonRDD.scala:48
filtered_by_function.count() =  2
filtered_by_function.collect() =  
[
 ('alex', 'Sunnyvale', 33), 
 ('mary', 'Cupertino', 66)
]