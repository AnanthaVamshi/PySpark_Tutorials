#--------------------------------------------------
# NOTE: 
#      when you view min_max_count.collect(),
#      the triplets (1, -1, 0) denotes the result
#      of an empty partition, which is filtered out
#--------------------------------------------------
./bin/spark-submit minmax_force_empty_partitions.py sample_numbers.txt

spark= <pyspark.sql.session.SparkSession object at 0x10b6a1710>

input_path= sample_numbers.txt

rdd= sample_numbers.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
rdd.count= 11
rdd.collect()= [u'23,24,22,44,66,77,44,44,555,666', u'12,4,555,66,67,68,57,55,56,45,45,45,66,77', u'34,35,36,97300,78,79', u'120,44,444,445,345,345,555', u'11,33,34,35,36,37,47,7777,8888,6666,44,55', u'10,11,44,66,77,78,79,80,90,98,99,100,102,103,104,105', u'6,7,8,9,10', u'8,9,10,12,12', u'7777', u'222,333,444,555,666,111,112,5,113,114', u'5555,4444,24']
rdd.getNumPartitions()= 17

min_max_count= PythonRDD[3] at RDD at PythonRDD.scala:48
min_max_count.count= 17
min_max_count.collect()= 
[
 (22, 666, 10), 
 (4, 555, 14), 
 (1, -1, 0), 
 (1, -1, 0), 
 (34, 97300, 6), 
 (44, 555, 7), 
 (11, 8888, 12), 
 (1, -1, 0), 
 (1, -1, 0), 
 (10, 105, 16), 
 (1, -1, 0), 
 (1, -1, 0), 
 (6, 12, 10), 
 (5, 7777, 11), 
 (1, -1, 0), 
 (24, 5555, 3), 
 (1, -1, 0)
]

final: (min, max, count)= ( 4 ,  97300 ,  89 )
