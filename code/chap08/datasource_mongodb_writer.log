export SPARK_HOME="/pyspark_book/spark-2.4.3"
export SPARK_PROG="/pyspark_book/code/chap08/datasource_mongodb_writer.py"
export MONGODB_COLLECTION_URI="mongodb://127.0.0.1/test.coll66"
export JAR1="/pyspark_book/code/jars/mongo-java-driver-3.8.2.jar"
export JAR2="/pyspark_book/code/jars/mongo-spark-connector_2.11-2.2.5.jar"
#
# run the PySpark program:
$SPARK_HOME/bin/spark-submit --jars "${JAR1},${JAR2}" $SPARK_PROG ${MONGODB_COLLECTION_URI}

mongodb_collection_uri :  mongodb://127.0.0.1/test.coll66

spark= <pyspark.sql.session.SparkSession object at 0x10495f7d0>

+-------+---------+---+
|name   |city     |age|
+-------+---------+---+
|Alex   |Ames     |50 |
|Gandalf|Cupertino|60 |
|Thorin |Sunnyvale|95 |
|Betty  |Ames     |78 |
|Brian  |Stanford |77 |
+-------+---------+---+

people.count() =  5
people.collect() =  
[
 Row(name=u'Alex', city=u'Ames', age=50), 
 Row(name=u'Gandalf', city=u'Cupertino', age=60), 
 Row(name=u'Thorin', city=u'Sunnyvale', age=95), 
 Row(name=u'Betty', city=u'Ames', age=78), 
 Row(name=u'Brian', city=u'Stanford', age=77)
]

root
 |-- name: string (nullable = true)
 |-- city: string (nullable = true)
 |-- age: long (nullable = true)