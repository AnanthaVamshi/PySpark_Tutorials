./bin/spark-submit rdd_creation_from_csv.py name_city_age.csv

spark= <pyspark.sql.session.SparkSession object at 0x1087bcba8>

input path :  name_city_age.csv
file_contents =
Alex,Ames,40
Betty,Ames,33
Alex,Ames,50
Betty,Stanford,45
Jeff,Sunnyvale,55
Bob,Sunnyvale,60
Terry,Stanford,75
David,Stanford,90
Don,Stanford,80

rdd = name_city_age.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
rdd.count =  9
rdd.collect() =  
[
 'Alex,Ames,40', 
 'Betty,Ames,33', 
 'Alex,Ames,50', 
 'Betty,Stanford,45', 
 'Jeff,Sunnyvale,55', 
 'Bob,Sunnyvale,60', 
 'Terry,Stanford,75', 
 'David,Stanford,90', 
 'Don,Stanford,80'
]

pairs = PythonRDD[3] at RDD at PythonRDD.scala:48
pairs.count =  9
pairs.collect() =  
[
 ('Ames', (40, 1)), 
 ('Ames', (33, 1)), 
 ('Ames', (50, 1)), 
 ('Stanford', (45, 1)), 
 ('Sunnyvale', (55, 1)), 
 ('Sunnyvale', (60, 1)), 
 ('Stanford', (75, 1)), 
 ('Stanford', (90, 1)), 
 ('Stanford', (80, 1))
]

sum_and_count = PythonRDD[9] at RDD at PythonRDD.scala:48
sum_and_count.count =  3
sum_and_count.collect() =  
[
 ('Stanford', (290, 4)), 
 ('Ames', (123, 3)), 
 ('Sunnyvale', (115, 2))
]

average_per_city = PythonRDD[11] at RDD at PythonRDD.scala:48
average_per_city.count =  3
average_per_city.collect() =  
[
 ('Stanford', 72.5), 
 ('Ames', 41.0), 
 ('Sunnyvale', 57.5)
]